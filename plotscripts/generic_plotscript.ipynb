{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from glob import glob\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HERE = os.path.abspath(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from AnalyticalLabware.analysis.spinsolve_spectrum import SpinsolveNMRSpectrum\n",
    "\n",
    "\n",
    "spectrum = SpinsolveNMRSpectrum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_code = \"terp-GA-acid-v1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_folder_paths(parent_folder):\n",
    "    folder_paths = []\n",
    "    for root, dirs, _ in tqdm(os.walk(parent_folder)):\n",
    "        for directory in dirs:\n",
    "            if \"processed\" not in directory and \"Enhanced\" not in directory:\n",
    "                folder_paths.append(os.path.join(root, directory))\n",
    "    return folder_paths\n",
    "\n",
    "\n",
    "parent_folder = os.path.join(\n",
    "    r\"C:\\Users\\rober\\Downloads\\raw spectra\\raw spectra\\NMR\", exp_code\n",
    ")\n",
    "datapaths = get_folder_paths(parent_folder)\n",
    "print(f\"Found {len(datapaths)} data files in {parent_folder}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just for getting the total monitoring time\n",
    "timestamps = [filepath.split(\"\\\\\")[-1][:15] for filepath in datapaths]\n",
    "times = sorted(\n",
    "    [datetime.strptime(timestamp, \"%Y%m%d_%H%M%S\") for timestamp in timestamps]\n",
    ")\n",
    "print(\"Total experiment time: \", times[-1] - times[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROCESSED = os.path.join(parent_folder, \"processed\")\n",
    "\n",
    "if not os.path.exists(PROCESSED):\n",
    "    os.makedirs(PROCESSED)\n",
    "\n",
    "for datapath in tqdm(datapaths):\n",
    "    spectrum.load_spectrum(data_path=datapath)\n",
    "    # spectrum.default_processing()\n",
    "\n",
    "    spectrum.apodization(function=\"gm\", g1=1.2, g2=4.5)\n",
    "    spectrum.zero_fill()\n",
    "    spectrum.fft()\n",
    "    # spectrum.reference_spectrum(new_position=1.8, reference='highest')\n",
    "    # spectrum.correct_baseline()\n",
    "    # spectrum.autophase()\n",
    "    # spectrum.correct_baseline()\n",
    "\n",
    "    spectrum.find_peaks(decimals=1, threshold=0.1)\n",
    "    spectrum.save_pickle(os.path.join(PROCESSED, os.path.basename(datapath)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_datafiles = sorted(glob(f\"{PROCESSED}/*\"))\n",
    "processed_datafiles = [file for file in processed_datafiles if file.endswith(\".pkl\")]\n",
    "timestamps = [filepath.split(\"\\\\\")[-1][:15] for filepath in processed_datafiles]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for datafile in tqdm(processed_datafiles):\n",
    "#   spectrum.load_data(datafile)\n",
    "#   spectrum.highpass_filter(threshold=100)\n",
    "#   spectrum.find_peaks(decimals=1, threshold=0.1)\n",
    "#   spectrum.save_data(datafile.removesuffix(\".pickle\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "# plotting only every xth spectrum for clarity\n",
    "for datafile in processed_datafiles[::4]:\n",
    "    spectrum = SpinsolveNMRSpectrum().from_pickle(datafile)\n",
    "    ax.plot(spectrum.x_data, spectrum.y_data.real)\n",
    "# ax.set_xlim(7,8)\n",
    "# ax.set_ylim(top=5e4)\n",
    "ax.invert_xaxis()\n",
    "ax.set_xlabel(\"Chemical Shift (ppm)\")\n",
    "ax.set_ylabel(\"Signal Intensity (a.u.)\")\n",
    "# plt.savefig(\"nmr_moni.svg\", transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty dictionary to store peak data\n",
    "peak_data = {}\n",
    "\n",
    "for datafile, timestamp in tqdm(zip(processed_datafiles, timestamps)):\n",
    "    spectrum.load_data(datafile)\n",
    "    spectrum.find_peaks(decimals=1, threshold=500)\n",
    "    spectrum.reference_spectrum(new_position=-60, reference=\"closest\")\n",
    "\n",
    "    # Create a list to store peak areas for this timestamp\n",
    "    timestamp_peak_areas = {}\n",
    "\n",
    "    for peak in spectrum.peaks[:, 0]:\n",
    "        # if peak <2:\n",
    "        #     continue\n",
    "        peak_id = peak.real\n",
    "        timestamp_peak_areas[peak_id] = spectrum.integrate_peak(peak)\n",
    "\n",
    "    # Add the timestamp data to the dictionary\n",
    "    peak_data[timestamp] = timestamp_peak_areas\n",
    "\n",
    "# Create the DataFrame from the dictionary\n",
    "peak_areas = pd.DataFrame.from_dict(peak_data, orient=\"index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# norm_peak_areas = peak_areas.div(peak_areas[0.3], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = peak_areas\n",
    "# df = norm_peak_areas\n",
    "times = sorted(\n",
    "    [datetime.strptime(timestamp, \"%Y%m%d_%H%M%S\") for timestamp in df.index]\n",
    ")\n",
    "print(\"Total experiment time: \", times[-1] - times[0])\n",
    "for column in df.columns:\n",
    "    # plt.plot(times, df[column], label=column)\n",
    "    plt.scatter(times, df[column], marker=\"o\", label=column)\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend(loc=\"center left\", bbox_to_anchor=(1, 0.5))\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Normalized peak area\")\n",
    "plt.title(f\"Automatic peak integration for {exp_code}\")\n",
    "# plt.xlim(times[0], times[-1])\n",
    "# plt.ylim(0,1.0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import linregress\n",
    "\n",
    "\n",
    "def detect_plateau_from_slope(\n",
    "    times, values, num_datapoints: int = 5, threshold: float = 1e-4\n",
    ") -> tuple:\n",
    "    \"\"\"\n",
    "    Detects a plateau in a list of values by calculating the slope of a linear regression\n",
    "    for the last num_datapoints and comparing it to a threshold value.\n",
    "    \"\"\"\n",
    "    for i in range(len(values) - num_datapoints):\n",
    "        if i < num_datapoints:\n",
    "            continue\n",
    "        slope = linregress(\n",
    "            range(num_datapoints),\n",
    "            np.array(values[i - num_datapoints : i]) / max(values),\n",
    "        ).slope\n",
    "        if abs(slope) < threshold:\n",
    "            elapsed_time = datetime.strptime(\n",
    "                times[i], \"%Y%m%d_%H%M%S\"\n",
    "            ) - datetime.strptime(times[0], \"%Y%m%d_%H%M%S\")\n",
    "            print(f\"Reaction has reached plateau after {elapsed_time}\")\n",
    "            return (elapsed_time, i)\n",
    "    return (None, len(values) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty dictionary to store peak data\n",
    "peak_data = {}\n",
    "peak_1 = 7.6  # -125\n",
    "peak_2 = 7.3  # -124.3\n",
    "\n",
    "for datafile, timestamp in tqdm(zip(processed_datafiles, timestamps)):\n",
    "    spectrum = SpinsolveNMRSpectrum().from_pickle(datafile)\n",
    "\n",
    "    # Create a list to store peak areas for this timestamp\n",
    "    timestamp_peak_areas = {}\n",
    "\n",
    "    timestamp_peak_areas[peak_1] = spectrum.integrate_area((peak_1 + 0.1, peak_1 - 0.1))\n",
    "    timestamp_peak_areas[peak_2] = spectrum.integrate_area((peak_2 + 0.2, peak_2 - 0.2))\n",
    "\n",
    "    # Add the timestamp data to the dictionary\n",
    "    peak_data[timestamp] = timestamp_peak_areas\n",
    "\n",
    "# Create the DataFrame from the dictionary\n",
    "peak_areas = pd.DataFrame.from_dict(peak_data, orient=\"index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update(\n",
    "    {\n",
    "        \"font.size\": 24,\n",
    "        \"axes.linewidth\": 3,\n",
    "        \"xtick.major.width\": 3,\n",
    "        \"ytick.major.width\": 3,\n",
    "        \"xtick.minor.width\": 3,\n",
    "        \"ytick.minor.width\": 3,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the ppm values for the peaks\n",
    "peak1_ppm = 7.6  # -125#-60#10.0#-61#7.8#4.88#2.2#7.6#11.5\n",
    "peak2_ppm = 7.3  # -124.3#-58#8.6#-59.5#7.5#2.6#4.6#7.3#9.6\n",
    "slope_threshold = 1e-3\n",
    "\n",
    "# create new dataframe with only the peaks of interest\n",
    "peak_areas_diagnostic = peak_areas[[peak1_ppm, peak2_ppm]]\n",
    "\n",
    "# drop the rows with NaN values from peak_areas\n",
    "peak_areas_diagnostic = peak_areas_diagnostic.dropna()\n",
    "\n",
    "# Extract the columns corresponding to the peaks at 11.5 ppm and 9.6 ppm\n",
    "peak1_column = peak_areas_diagnostic[peak1_ppm]\n",
    "peak2_column = peak_areas_diagnostic[peak2_ppm]\n",
    "\n",
    "# Calculate the ratio of the peaks at each timestamp\n",
    "peak_ratio = peak2_column / (peak1_column + peak2_column)\n",
    "\n",
    "elapsed_time, plateau_index = detect_plateau_from_slope(\n",
    "    peak_areas_diagnostic.index, peak_ratio, num_datapoints=5, threshold=slope_threshold\n",
    ")\n",
    "\n",
    "formatted_times = sorted(\n",
    "    [\n",
    "        datetime.strptime(timestamp, \"%Y%m%d_%H%M%S\")\n",
    "        for timestamp in peak_areas_diagnostic.index\n",
    "    ]\n",
    ")\n",
    "formatted_times = [\n",
    "    (timestamp - formatted_times[0]).total_seconds() / 60\n",
    "    for timestamp in formatted_times\n",
    "]\n",
    "# formatted_times = [(datetime.strptime(timestamp, '%Y%m%d_%H%M%S') - datetime.strptime(peak_areas_diagnostic.index[0], '%Y%m%d_%H%M%S')).total_seconds() for timestamp in peak_areas_diagnostic.index]\n",
    "\n",
    "# Plot the peak ratio over time\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.plot(formatted_times, peak_ratio, marker=\"o\", linestyle=\"None\", markersize=10)\n",
    "plt.axvline(\n",
    "    x=formatted_times[plateau_index],\n",
    "    color=\"r\",\n",
    "    linestyle=\"--\",\n",
    "    label=f\"Plateau after {elapsed_time}\",\n",
    ")\n",
    "plt.xlabel(\"Time / min\")\n",
    "plt.ylabel(\"Conversion\")\n",
    "# format yticks to only have one decimal place\n",
    "plt.yticks(np.round(np.arange(0.7, 0.9, 0.05), 2))\n",
    "plt.ylim(min(peak_ratio) - 0.01, max(peak_ratio) + 0.01)\n",
    "# plt.title(f'Conversion for experiment {exp_code}')\n",
    "# plt.xticks(rotation=45)\n",
    "plt.legend()\n",
    "# plt.savefig(exp_code + '_conversion.eps', format='eps', bbox_inches='tight')\n",
    "plt.savefig(\n",
    "    exp_code + \"_conversion.svg\", bbox_inches=\"tight\", dpi=300, transparent=True\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PPM_PRECISION = 4\n",
    "SOLVENT_PEAKS = [1.16, 3.44]\n",
    "# HIGHPASS = 1000\n",
    "\n",
    "# ppm in reverse order because NMR convention is to have ppm decrease from left to right\n",
    "# start_ppm, end_ppm = 0.0, -200.0\n",
    "start_ppm, end_ppm = 12.0, 0.0\n",
    "\n",
    "timestamps = [filepath.split(\"\\\\\")[-1][:15] for filepath in processed_datafiles]\n",
    "\n",
    "# Save spectrum objects in a dict\n",
    "spectra = {}\n",
    "\n",
    "\n",
    "def highpass_filter(spectrum: SpinsolveNMRSpectrum, threshold: float, inplace=True):\n",
    "    s = spectrum\n",
    "    s.y_data = np.where(s.y_data.real > threshold, s.y_data.real, 0)\n",
    "    return s\n",
    "\n",
    "\n",
    "def cut(spectrum: SpinsolveNMRSpectrum, low_ppm: float, high_ppm: float, inplace=True):\n",
    "    s = spectrum\n",
    "    selector = (s.x_data < low_ppm) | (s.x_data > high_ppm)\n",
    "    s.x_data = s.x_data[selector]\n",
    "    s.y_data = s.y_data[selector]\n",
    "    return s\n",
    "\n",
    "\n",
    "def trim(spectrum: SpinsolveNMRSpectrum, low_ppm: float, high_ppm: float, inplace=True):\n",
    "    s = spectrum\n",
    "    selector = (s.x_data >= low_ppm) & (s.x_data <= high_ppm)\n",
    "    s.x_data = s.x_data[selector]\n",
    "    s.y_data = s.y_data[selector]\n",
    "    return s\n",
    "\n",
    "\n",
    "# process first spectrum individually to get common x-axis\n",
    "spectrum = SpinsolveNMRSpectrum().from_pickle(processed_datafiles[0])\n",
    "trim(spectrum, end_ppm, start_ppm)\n",
    "for solvent_peak in SOLVENT_PEAKS:\n",
    "    cut(spectrum, solvent_peak - 0.5, solvent_peak + 0.5)\n",
    "\n",
    "common_x_axis = np.round(spectrum.x_data, decimals=PPM_PRECISION)\n",
    "\n",
    "print(\"Creating array of spectra...\")\n",
    "spectra_array = np.zeros((len(processed_datafiles), len(common_x_axis)))\n",
    "for i in tqdm(range(len(processed_datafiles))):\n",
    "    spectrum = SpinsolveNMRSpectrum().from_pickle(processed_datafiles[i])\n",
    "    # highpass_filter(spectrum, HIGHPASS)\n",
    "    indices = np.digitize(common_x_axis, spectrum.x_data)\n",
    "    spectra_array[i] = spectrum.y_data.real[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.integrate import trapezoid\n",
    "\n",
    "nmr_error = 1e3\n",
    "\n",
    "\n",
    "# Calculate Jaccard index for pair of spectra\n",
    "def jaccard_next_two_spectra(index_1, index_2, reference_spectrum=None):\n",
    "    if reference_spectrum is None:\n",
    "        spectrum1_y = spectra_array[index_1]\n",
    "    else:\n",
    "        spectrum1_y = reference_spectrum\n",
    "    spectrum2_y = spectra_array[index_2]\n",
    "    union, intersection = [], []\n",
    "    for y1, y2 in zip(spectrum1_y, spectrum2_y):\n",
    "        if abs(y1 - y2) < nmr_error:\n",
    "            union.append(y2)\n",
    "            intersection.append(y2)\n",
    "        else:\n",
    "            union.append(max(y1, y2))\n",
    "            intersection.append(min(y1, y2))\n",
    "\n",
    "    intersection_area = trapezoid(intersection, common_x_axis)\n",
    "    union_area = trapezoid(union, common_x_axis)\n",
    "    return intersection_area / union_area\n",
    "\n",
    "\n",
    "print(\"Calculating Jaccard indices...\")\n",
    "jaccards = {}\n",
    "for i in tqdm(range(len(timestamps))):\n",
    "    jaccards[timestamps[i]] = jaccard_next_two_spectra(0, i)\n",
    "\n",
    "print(\"Calculating reverse Jaccard indices...\")\n",
    "reverse_jaccards = {}\n",
    "for i in tqdm(range(len(timestamps))):\n",
    "    reverse_jaccards[timestamps[i]] = jaccard_next_two_spectra(i, len(timestamps) - 1)\n",
    "\n",
    "print(\"Calclating neighbor Jaccard indices...\")\n",
    "neighbor_jaccards = {}\n",
    "for i in tqdm(range(len(timestamps) - 1)):\n",
    "    neighbor_jaccards[timestamps[i]] = jaccard_next_two_spectra(i, i + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "times = [datetime.strptime(timestamp, \"%Y%m%d_%H%M%S\") for timestamp in jaccards.keys()]\n",
    "plt.figure(figsize=(24, 6), dpi=100)\n",
    "plt.plot(\n",
    "    times,\n",
    "    jaccards.values(),\n",
    "    marker=\"o\",\n",
    "    label=f\"Jaccard index {start_ppm} to {end_ppm} ppm\",\n",
    ")\n",
    "plt.plot(\n",
    "    times,\n",
    "    reverse_jaccards.values(),\n",
    "    marker=\"o\",\n",
    "    label=f\"Reverse Jaccard index {start_ppm} to {end_ppm} ppm\",\n",
    ")\n",
    "plt.plot(\n",
    "    times[:-1],\n",
    "    neighbor_jaccards.values(),\n",
    "    marker=\"o\",\n",
    "    label=f\"Neighbor Jaccard index {start_ppm} to {end_ppm} ppm\",\n",
    ")\n",
    "\n",
    "plt.xticks(times[::2], times[::2], rotation=90)\n",
    "plt.legend()\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Jaccard index\")\n",
    "plt.title(f\"Jaccard index for {exp_code}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elapsed_time, plateau_index = detect_plateau_from_slope(\n",
    "    list(jaccards.keys()), list(jaccards.values()), num_datapoints=5, threshold=1e-3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "times = [datetime.strptime(timestamp, \"%Y%m%d_%H%M%S\") for timestamp in jaccards.keys()]\n",
    "times_min = [(time - times[0]).total_seconds() / 60 for time in times]\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.plot(\n",
    "    times_min,\n",
    "    jaccards.values(),\n",
    "    marker=\"o\",\n",
    "    label=f\"Jaccard index {start_ppm} to {end_ppm} ppm\",\n",
    "    linestyle=\"None\",\n",
    "    markersize=10,\n",
    ")\n",
    "# plt.plot(times_min, reverse_jaccards.values(), marker='o', label=f'Reverse Jaccard index {start_ppm} to {end_ppm} ppm')\n",
    "# plot vertical line at plateau index\n",
    "plt.axvline(\n",
    "    x=times_min[plateau_index],\n",
    "    color=\"r\",\n",
    "    linestyle=\"--\",\n",
    "    label=f\"Plateau after {elapsed_time}\",\n",
    ")\n",
    "\n",
    "plt.legend().get_frame().set_linewidth(3)\n",
    "\n",
    "plt.xlabel(\"Time / min\")\n",
    "plt.ylabel(\"Jaccard index\")\n",
    "plt.yticks(np.round(np.arange(0.7, 1.0, 0.1), 1))\n",
    "plt.ylim(min(jaccards.values()) - 0.01, max(jaccards.values()) + 0.01)\n",
    "\n",
    "# plt.title(f'Jaccard index for {exp_code}')#\\n excluding signals from {SOLVENT_PEAKS} ppm')\n",
    "# plt.savefig(exp_code + '_jaccard_error_0.eps', format='eps', bbox_inches='tight')\n",
    "plt.savefig(\n",
    "    f\"figures/{exp_code}_jaccard.svg\", bbox_inches=\"tight\", dpi=300, transparent=True\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\n",
    "    exp_code + \"plotdata_jaccard.csv\",\n",
    "    np.column_stack((times_min, list(jaccards.values()))),\n",
    "    header=\"Time, Jaccard index\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import curve_fit\n",
    "\n",
    "PLATEAU_THRESHOLD_MARGIN = 0.01\n",
    "PLATEAU_DATAPOINTS = 5\n",
    "\n",
    "# exclude outliers with jaccard index < 0.1 from jaccards dict\n",
    "jaccards_filtered = {k: v for k, v in jaccards.items() if v >= 0.0}\n",
    "times = [\n",
    "    datetime.strptime(timestamp, \"%Y%m%d_%H%M%S\")\n",
    "    for timestamp in jaccards_filtered.keys()\n",
    "]\n",
    "\n",
    "datapoints = np.array(list(jaccards_filtered.values()))\n",
    "\n",
    "\n",
    "# Define fit function for reaction of first order (exponential decay)\n",
    "def exp_decay(x, a, b, c):\n",
    "    return a * np.exp(-b * x) + c\n",
    "\n",
    "\n",
    "# Define fit function for reaction of second order\n",
    "def second_order(x, a, b, c):\n",
    "    return 1 / (1 / a + b * x) + c\n",
    "\n",
    "\n",
    "# Convert the timestamps to elapsed time in seconds\n",
    "elapsed_time_sec = np.array([(time - times[0]).total_seconds() for time in times])\n",
    "\n",
    "# Fit the exponential decay function to the Jaccard index time series\n",
    "popt1, pcov1 = curve_fit(exp_decay, elapsed_time_sec, datapoints, p0=(1, 1e-3, 0.5))\n",
    "\n",
    "# Fit the second order function to the Jaccard index time series\n",
    "popt2, pcov2 = curve_fit(second_order, elapsed_time_sec, datapoints, p0=(1, 1e-3, 0.5))\n",
    "\n",
    "# find out where the plateau starts\n",
    "# check if 5 datapoints in a row are within 1% of the c parameter of the fitted function\n",
    "# if so, the reaction has reached the plateau\n",
    "plateau_index = len(datapoints) - 1\n",
    "elapsed_time = None\n",
    "for i in range(len(datapoints) - PLATEAU_DATAPOINTS):\n",
    "    if i < PLATEAU_DATAPOINTS:\n",
    "        continue\n",
    "    if all(\n",
    "        abs(datapoints[i - PLATEAU_DATAPOINTS : i] - popt1[2])\n",
    "        < PLATEAU_THRESHOLD_MARGIN\n",
    "    ):\n",
    "        print(i)\n",
    "        print(f\"Reaction has reached plateau at {list(jaccards.keys())[i]}\")\n",
    "        plateau_index = i\n",
    "        elapsed_time = times[plateau_index] - times[0]\n",
    "        break\n",
    "\n",
    "# Plot the fitted function\n",
    "plt.plot(\n",
    "    times_min,\n",
    "    exp_decay(elapsed_time_sec, *popt1),\n",
    "    \"r-\",\n",
    "    label=\"fit 1st: a=%.3f, b=%.3e, c=%.3f\" % tuple(popt1),\n",
    ")\n",
    "# plt.plot(times, second_order(elapsed_time_sec, *popt2), 'g-', label='fit 2nd: a=%.3f, b=%.3e, c=%.3f' % tuple(popt2))\n",
    "plt.plot(\n",
    "    times_min, jaccards.values(), marker=\"o\", linestyle=\"None\"\n",
    ")  # , label=f'Jaccard index {start_ppm} to {end_ppm} ppm')\n",
    "plt.axvline(\n",
    "    x=times_min[plateau_index],\n",
    "    color=\"r\",\n",
    "    linestyle=\"--\",\n",
    "    label=f\"Plateau after {elapsed_time}\",\n",
    ")\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend()\n",
    "plt.xlabel(\"Time / min\")\n",
    "plt.ylabel(\"Jaccard index\")\n",
    "plt.title(\n",
    "    f\"Jaccard index for {exp_code}\"\n",
    ")  # \\n excluding signals from {SOLVENT_PEAKS} ppm')\n",
    "# plt.savefig(exp_code + '_jaccard_exp.eps', format='eps', bbox_inches='tight')\n",
    "# plt.savefig(exp_code + '_jaccard_exp.png', bbox_inches='tight', transparent=True, dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Calculate R-squared value for the first fit\n",
    "r2_1 = r2_score(datapoints, exp_decay(elapsed_time_sec, *popt1))\n",
    "\n",
    "# Calculate R-squared value for the second fit\n",
    "r2_2 = r2_score(datapoints, second_order(elapsed_time_sec, *popt2))\n",
    "\n",
    "print(\"R-squared value for the first fit:\", r2_1)\n",
    "# parameters for first fit including errors\n",
    "print(\"a = \", popt1[0], \"+/-\", np.sqrt(pcov1[0, 0]))\n",
    "print(\"b = \", popt1[1], \"+/-\", np.sqrt(pcov1[1, 1]))\n",
    "print(\"c = \", popt1[2], \"+/-\", np.sqrt(pcov1[2, 2]))\n",
    "\n",
    "print(\"R-squared value for the second fit:\", r2_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transposed = spectra_array.T\n",
    "nmr_stds = np.zeros(len(transposed))\n",
    "nmr_cvs = np.zeros(len(transposed))\n",
    "nmr_rel_stds = np.zeros(len(transposed))\n",
    "\n",
    "for i, y_values in enumerate(transposed):\n",
    "    nmr_stds[i] = np.std(y_values)\n",
    "    nmr_cvs[i] = np.std(y_values) / np.mean(y_values)\n",
    "    nmr_rel_stds[i] = np.std(y_values) / np.max(y_values)\n",
    "\n",
    "print(\n",
    "    f\"Mean NMR std: {np.mean(nmr_stds)}, mean NMR cv: {np.mean(nmr_cvs)}, mean NMR rel std: {np.mean(nmr_rel_stds)}\"\n",
    ")\n",
    "print(\n",
    "    f\"Median NMR std: {np.median(nmr_stds)}, median NMR cv: {np.median(nmr_cvs)}, median NMR rel std: {np.median(nmr_rel_stds)}\"\n",
    ")\n",
    "print(\n",
    "    f\"Max NMR std: {np.max(nmr_stds)}, max NMR cv: {np.max(nmr_cvs)}, max NMR rel std: {np.max(nmr_rel_stds)}\"\n",
    ")\n",
    "print(\n",
    "    f\"Min NMR std: {np.min(nmr_stds)}, min NMR cv: {np.min(nmr_cvs)}, min NMR rel std: {np.min(nmr_rel_stds)}\"\n",
    ")\n",
    "print(\n",
    "    f\"Max std as percentage of overall max value: {np.max(nmr_stds) / np.max(np.max(spectra_array)) * 100}%\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(transposed[0], bins=10)\n",
    "plt.xlabel(\"Value\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(f\"Histogram of Row {1}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "times = [datetime.strptime(timestamp, \"%Y%m%d_%H%M%S\") for timestamp in jaccards.keys()]\n",
    "times_min = [(time - times[0]).total_seconds() / 60 for time in times]\n",
    "datapoints = np.array(list(jaccards.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming y is your data\n",
    "window_size = 20\n",
    "moving_avg = np.convolve(datapoints, np.ones(window_size) / window_size, mode=\"valid\")\n",
    "\n",
    "plt.plot(times_min[: len(moving_avg)], moving_avg)\n",
    "plt.xlabel(\"X-axis\")\n",
    "plt.ylabel(\"Smoothed Y-axis\")\n",
    "plt.title(\"Moving Average\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.nonparametric.smoothers_lowess import lowess\n",
    "\n",
    "# Assuming x and y are your data\n",
    "smooth_data = lowess(datapoints, times_min, frac=0.1)\n",
    "\n",
    "plt.plot(times_min, smooth_data[:, 1])\n",
    "plt.xlabel(\"X-axis\")\n",
    "plt.ylabel(\"Smoothed Y-axis\")\n",
    "plt.title(\"LOESS Smoothing\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr\n",
    "\n",
    "# Assuming x and y are your data\n",
    "correlation, _ = spearmanr(times_min, datapoints)\n",
    "print(f\"Spearman's Rank Correlation: {correlation}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spear_corr = []\n",
    "for index in range(len(times_min)):\n",
    "    correlation, _ = spearmanr(times_min[:index], datapoints[:index])\n",
    "    spear_corr.append(correlation)\n",
    "\n",
    "plt.plot(\n",
    "    times_min,\n",
    "    spear_corr,\n",
    "    marker=\"o\",\n",
    "    label=f\"Spearman correlation {start_ppm} to {end_ppm} ppm\",\n",
    ")\n",
    "plt.xlabel(\"Time / min\")\n",
    "plt.ylabel(\"Spearman correlation\")\n",
    "plt.title(f\"Spearman correlation for {exp_code}\")\n",
    "# plt.xticks(rotation=45)\n",
    "plt.legend()\n",
    "# plt.ylim(-1.1,0)\n",
    "# plt.savefig(exp_code + '_spearman.eps', format='eps', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\n",
    "    \"plotdata.csv\", np.column_stack((times_min, spear_corr)), header=\"Time, Spearman\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_datapoints = 5\n",
    "spear_corr = []\n",
    "for index in range(num_datapoints, len(elapsed_time_sec)):\n",
    "    correlation, _ = spearmanr(\n",
    "        elapsed_time_sec[index - num_datapoints : index],\n",
    "        datapoints[index - num_datapoints : index],\n",
    "    )\n",
    "    spear_corr.append(correlation)\n",
    "\n",
    "plt.plot(\n",
    "    elapsed_time_sec[num_datapoints:],\n",
    "    spear_corr,\n",
    "    marker=\"o\",\n",
    "    label=f\"Spearman correlation {start_ppm} to {end_ppm} ppm\",\n",
    ")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Spearman correlation\")\n",
    "plt.title(f\"Spearman correlation for {exp_code}\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend()\n",
    "# plt.ylim(-1.1,0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "U, S, V = np.linalg.svd(spectra_array, full_matrices=False)\n",
    "\n",
    "print(U.shape, S.shape, V.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstructed_spectra = np.transpose(V) * S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstructed_spectra = V[:, : S.shape[0]] * S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstructed_timepoints = U * S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(2, 1, sharex=True)\n",
    "\n",
    "ax1.plot(reconstructed_timepoints[:, 0])\n",
    "ax1.set_ylabel(\"SVD 0\")\n",
    "\n",
    "ax2.plot(reconstructed_timepoints[:, 1])\n",
    "ax2.set_ylabel(\"SVD 1\")\n",
    "\n",
    "plt.xlabel(\"experiment number\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(2, 1, sharex=True)\n",
    "\n",
    "ax1.plot(common_x_axis, reconstructed_spectra[:, 0])\n",
    "ax1.set_ylabel(\"SVD 0\")\n",
    "\n",
    "ax2.plot(common_x_axis, reconstructed_spectra[:, 1])\n",
    "ax2.set_ylabel(\"SVD 1\")\n",
    "\n",
    "# invert x axis\n",
    "ax1.set_xlim(ax1.get_xlim()[::-1])\n",
    "plt.xlabel(\"chemical shift / ppm\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(S)\n",
    "# make y axes logarithmic\n",
    "plt.yscale(\"log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_peak(\n",
    "    wavelengths: np.array, intensities: np.array, peak: float, width: float\n",
    ") -> tuple[int, float, float]:\n",
    "    if len(wavelengths) != len(intensities):\n",
    "        raise ValueError(\"x_values and intensity must have the same length\")\n",
    "    selector = (wavelengths >= peak - width) & (wavelengths <= peak + width)\n",
    "    index_of_max_intensity = intensities[selector].argmax()\n",
    "    max_wavelength = wavelengths[selector][index_of_max_intensity]\n",
    "    max_intensity = intensities[selector][index_of_max_intensity]\n",
    "    return index_of_max_intensity, max_wavelength, max_intensity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peak_shifts = []\n",
    "for datafile in tqdm(processed_datafiles):\n",
    "    spectrum.load_data(datafile)\n",
    "    _, shift, _ = find_peak(spectrum.x, spectrum.y, peak=3.5, width=0.5)\n",
    "    peak_shifts.append(shift)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(times_min, peak_shifts, marker=\".\", linestyle=\"None\")\n",
    "plt.xlabel(\"Time / min\")\n",
    "plt.ylabel(\"Peak position / ppm\")\n",
    "plt.title(\"Tracking the peak between 3 and 4 ppm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spec_sim",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
